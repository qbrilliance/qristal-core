{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a828395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qb.core\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.optim import LBFGS, SGD, Adam, RMSprop\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(sys.path[0])),'plugins','optimisation_modules','QML'))\n",
    "import qb_qml \n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import deque\n",
    "# import torchviz\n",
    "import dataFormat\n",
    "from subjectJoins import SubjectJoins\n",
    "from imports import layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a70f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time = 09:22:34\n",
      "Settings:  {'features': 'simple', 'encoding': 'rx', 'reuploading': True, 'reps': 5, 'calc': 'yz', 'entangleType': 'circular', 'entangle': 'cx', 'reward': 'rational', 'numEpisodes': 200, 'optimizer': 'adam', 'lr': 0.01}\n",
      "Episode: 0, loss: 0.209, Reward : 0.006\n",
      "Episode: 1, loss: 0.370, Reward : 0.019\n",
      "Episode: 2, loss: 0.450, Reward : 0.346\n",
      "Episode: 3, loss: 0.247, Reward : 0.261\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start Time =\", current_time)\n",
    "t1 = time.time()\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# default settings\n",
    "settings = {\n",
    "    \"features\": \"simple\",  \n",
    "    \"encoding\": \"rx\",\n",
    "    \"reuploading\" : True,\n",
    "    \"reps\" : 5,\n",
    "    \"calc\" : \"yz\",\n",
    "    \"entangleType\": \"circular\",\n",
    "    \"entangle\" : \"cx\",\n",
    "    \"reward\" : \"rational\",\n",
    "    \"numEpisodes\" : 200,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\" : 0.01\n",
    "}\n",
    "\n",
    "num_qubits = 4\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# ----------------- Begin quantum section ----------------------\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "calcCircuits = qb_qml.stringToCircuitList(settings[\"calc\"])\n",
    "reps = settings[\"reps\"]\n",
    "num_params = num_qubits*reps*len(calcCircuits)\n",
    "\n",
    "# Connect to PyTorch\n",
    "initial_weights = np.pi*(2*np.random.rand(num_params) - 1)\n",
    "quantum_nn = qb_qml.QuantumLayer(init_weights=initial_weights)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# ------------------- End quantum section ----------------------\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Define neural network and loss\n",
    "\n",
    "normLayer = layer.NormLayer()\n",
    "model = torch.nn.Sequential(quantum_nn, normLayer)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "Uncomment code below to test if the model works + visualize the model\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# testtensor = torch.Tensor([1.0]*num_qubits)\n",
    "# testresult = model(testtensor)\n",
    "# print([param for param in model.named_parameters()]) # Printing all parameters that can have weights varied\n",
    "# torchviz.make_dot(testresult,params=dict(model.named_parameters())) # To visualise the computational graph of the NN\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Code for running the actual model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# create reinforcement environment\n",
    "env = SubjectJoins()\n",
    "env.setRewardType(settings[\"reward\"])\n",
    "env.setFeatureType(settings[\"features\"])\n",
    "\n",
    "# load data\n",
    "i_filename= \"data.csv\"\n",
    "with open(i_filename, \"r\") as input:\n",
    "    env.load_data(input)\n",
    "\n",
    "# choose optimizer\n",
    "if settings[\"optimizer\"]==\"adam\":\n",
    "    optimizer = Adam(model.parameters(), lr=settings[\"lr\"], amsgrad=True)\n",
    "# # elif settings[\"optimizer\"]==\"SGD\":\n",
    "#      optimizer = SGD(model.parameters(), lr=settings[\"lr\"], momentum=0.9)\n",
    "# # else:\n",
    "# #     optimizer = Adam(model.parameters(), lr=settings[\"lr\"])\n",
    "\n",
    "logInterval = 100\n",
    "numEpisodes = settings[\"numEpisodes\"]\n",
    "\n",
    "print(\"Settings: \", settings)\n",
    "\n",
    " # initialize variables for live evaluation\n",
    "rewards = []\n",
    "av_rewards = []\n",
    "best_score = 0\n",
    "rewardList = deque(maxlen=40)\n",
    "\n",
    "# # initial observation\n",
    "state = env.reset()  \n",
    "\n",
    "loss_arr = np.zeros(numEpisodes)\n",
    "reward_arr = np.zeros(numEpisodes)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "# train the agent\n",
    "for episode in range(numEpisodes):\n",
    "    # learn a new state\n",
    "    state = Tensor(state)\n",
    "    prediction = model(state)\n",
    "    selected = prediction.argmax()\n",
    "    state, rewards, done, info = env.step(selected)\n",
    "    # pad rewards to 16\n",
    "    rewards.append(0)\n",
    "    reward = rewards[selected]\n",
    "\n",
    "    # calculate average for console output\n",
    "    rewardList.append(reward)\n",
    "    sumRewards = 0\n",
    "    countRewards = 0\n",
    "    for reward in rewardList:\n",
    "        sumRewards+= reward\n",
    "        countRewards+= 1\n",
    "    averageReward = sumRewards/countRewards\n",
    "\n",
    "    # optimize\n",
    "    loss= 0\n",
    "    loss = loss_fn(prediction, torch.Tensor(rewards))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_arr[episode] = loss.item()\n",
    "    reward_arr[episode] = averageReward\n",
    "    # print current result\n",
    "    print(\"Episode: {}, loss: {:.3f}, Reward : {:.3f}\".format(episode, loss_arr[episode] , averageReward), end=\"\\n\")\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"End Time =\", current_time)\n",
    "t2 = time.time()\n",
    "print(\"Time Taken = \" + str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028048d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(numEpisodes), loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83454f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(numEpisodes),reward_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260e565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qc4db",
   "language": "python",
   "name": "qc4db"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
